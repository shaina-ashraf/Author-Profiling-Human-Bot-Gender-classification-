{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cc-Arguments-Mining.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2LhjxTwAmxC6",
        "uT-_Rec9ip_j"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaina-ashraf/Author-Profiling-Human-Bot-Gender-classification-/blob/main/cc_Arguments_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "2LhjxTwAmxC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "\n",
        "from copy import copy\n",
        "import sklearn\n",
        "import bz2\n",
        "import pickle\n",
        "import unicodedata\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import keras\n",
        "import numpy as np\n",
        "import keras.backend as keras_backend\n",
        "\n",
        "from os import listdir\n",
        "from xml.etree import ElementTree\n",
        "from keras import Input\n",
        "from keras.layers import merge, Lambda, Activation, Convolution1D\n",
        "from keras.layers.core import Dense, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import sequence as sequence_module\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Activation\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.metrics.confusionmatrix import ConfusionMatrix\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk import RegexpParser\n",
        "\n",
        "from theano.scalar import float32\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FkXtCfssiASj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a81964d5-8bce-4714-b6fd-25fb33757bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization and Words Embeddings\n"
      ],
      "metadata": {
        "id": "uT-_Rec9ip_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_tokenizer(text):\n",
        "  return sent_tokenize(text)\n",
        "\n",
        "def generate_pos_tags(text, patterns=\"\"):\n",
        "\n",
        "  tokens_tag = pos_tag(text.split())\n",
        "  return tokens_tag\n",
        "\n",
        "def tokenize(s):\n",
        "    sentence_splitter = TweetTokenizer()\n",
        "    tokens = sentence_splitter.tokenize(s)\n",
        "    result = []\n",
        "    for word in tokens:\n",
        "        result.append(unicodedata.normalize('NFKD', word))\n",
        "    return result\n",
        "\n",
        "\n",
        "def load_embeddings(saved_embeddings):\n",
        "    \n",
        "    (frequencies, word_embedding_map) = pickle.load(bz2.BZ2File(saved_embeddings, 'r'), encoding='latin1')\n",
        "    return frequencies, word_embedding_map\n",
        "\n",
        "\n",
        "def dictionary_and_embeddings_to_indices(word_frequencies, embeddings):\n",
        "    \"\"\"\n",
        "    Sort words by frequency, adds offset (3 items), maps word indices to embeddings and generate embeddings\n",
        "    for padding, start of sequence, and OOV\n",
        "    :param word_frequencies: dict (word: frequency)\n",
        "    :param embeddings: dict (word: embeddings array)\n",
        "    :return: word_to_indices_map (word: index), word_index_to_embeddings_map (index: embeddings)\n",
        "    \"\"\"\n",
        "\n",
        "    # sort word frequencies from the most common ones\n",
        "    sorted_word_frequencies_keys = sorted(word_frequencies, key=word_frequencies.get, reverse=True)\n",
        "\n",
        "    word_to_indices_map = dict()\n",
        "    word_index_to_embeddings_map = dict()\n",
        "\n",
        "    # offset for all words so their indices don't start with 0\n",
        "    # 0 is reserved for padding\n",
        "    # 1 is reserved for start of sequence\n",
        "    # 2 is reserved for OOV\n",
        "    offset = 3\n",
        "    \n",
        "    # we also need to initialize embeddings for 0, 1, and 2\n",
        "    # what is the dimension first?\n",
        "    embedding_dimension = len(list(embeddings.values())[0])\n",
        "\n",
        "    # for padding we add all zeros\n",
        "    vector_padding = [0.0] * embedding_dimension\n",
        "\n",
        "    # for start of sequence and OOV we add random vectors\n",
        "    vector_start_of_sequence = 2 * 0.1 * np.random.rand(embedding_dimension) - 0.1\n",
        "    vector_oov = 2 * 0.1 * np.random.rand(embedding_dimension) - 0.1\n",
        "\n",
        "    # and add them to the embeddings map\n",
        "    word_index_to_embeddings_map[0] = vector_padding\n",
        "    word_index_to_embeddings_map[1] = vector_start_of_sequence\n",
        "    word_index_to_embeddings_map[2] = vector_oov\n",
        "\n",
        "    # iterate with index\n",
        "    for idx, word in enumerate(sorted_word_frequencies_keys):\n",
        "        # print idx, word\n",
        "\n",
        "        new_index = idx + offset\n",
        "\n",
        "        # update maps\n",
        "        word_to_indices_map[word] = new_index\n",
        "\n",
        "        if embeddings.get(word) is not None:\n",
        "            word_index_to_embeddings_map[new_index] = embeddings.get(word)\n",
        "        else:\n",
        "            # fix embedding entries which are None with OOV vector\n",
        "            word_index_to_embeddings_map[new_index] = vector_oov\n",
        "\n",
        "    return word_to_indices_map, word_index_to_embeddings_map\n",
        "\n",
        "\n",
        "def load_wordembedding_file(serialized_file='/content/gdrive/MyDrive/WMCA/data/vocabulary.embeddings.all.pkl.bz2'):\n",
        "    # load\n",
        "    print(\"Load ALL.. ****\")\n",
        "    print(serialized_file)\n",
        "    freq, embeddings_map = load_embeddings(serialized_file)\n",
        "    word_to_indices_map, word_index_to_embeddings_map = dictionary_and_embeddings_to_indices(freq, embeddings_map)\n",
        "    return word_to_indices_map, word_index_to_embeddings_map"
      ],
      "metadata": {
        "id": "X4eOQ-eRi6Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loading Data from csv files"
      ],
      "metadata": {
        "id": "Nl2l7FOvnJNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_csv(directory, file_name, word_to_indices_map, max_words=None, reduced_label_set=False):\n",
        "\n",
        "    file = open(directory +\"/\"+ file_name, 'r')\n",
        "    lines = file.readlines()\n",
        "    del lines[0]\n",
        "\n",
        "    x_vectors = []\n",
        "    y_labels = []\n",
        "    id_vector = []\n",
        "\n",
        "    for line in lines:\n",
        "        print(\"********* START ***********\")\n",
        "        line_split_list = line.split('\\t')\n",
        "        if len(line_split_list) >= 4:\n",
        "          arg_id, label, arg_1, arg_2 = line_split_list\n",
        "        else:\n",
        "          continue\n",
        "\n",
        "        # print(arg_id +\"-\",label+\"->>>\", arg_1+\"->>>\", arg_2+\"-\")\n",
        "\n",
        "        id_vector.append(arg_id)\n",
        "        arg_1_tokens = tokenize(arg_1)\n",
        "        arg_2_tokens = tokenize(arg_2)\n",
        "\n",
        "        arg_1_indices = [word_to_indices_map.get(word, 2) for word in arg_1_tokens]\n",
        "        arg_2_indices = [word_to_indices_map.get(word, 2) for word in arg_2_tokens]\n",
        "\n",
        "        arg_1_sent_tokens = sentence_tokenizer(arg_1)\n",
        "        arg_2_sent_tokens = sentence_tokenizer(arg_2)\n",
        "\n",
        "        arg_1_len = len(arg_1)\n",
        "        arg_2_len = len(arg_2)\n",
        "\n",
        "        arg_1_pos_tags = generate_pos_tags(arg_1)\n",
        "        arg_2_pos_tags = generate_pos_tags(arg_2)\n",
        "\n",
        "\n",
        "        # join them into one vector, start with 1 for start_of_sequence, add also 1 in between\n",
        "        x = [1] + arg_1_indices + [1] + arg_2_indices\n",
        "\n",
        "        # map class to vector\n",
        "        all_labels = [\"o5_1\", \"o5_2\", \"o5_3\", \"o6_1\", \"o6_2\", \"o6_3\", \"o7_1\", \"o7_2\", \"o7_3\", \"o7_4\", \"o8_1\", \"o8_4\",\n",
        "                      \"o8_5\", \"o9_1\", \"o9_2\", \"o9_3\", \"o9_4\"]\n",
        "        if reduced_label_set:\n",
        "            all_labels = [\"o5\", \"o6\", \"o7\"]\n",
        "\n",
        "\n",
        "        # zeros vector y\n",
        "        y = np.zeros(len(all_labels))\n",
        "        # split label by comma\n",
        "\n",
        "        for l in label.split(','):\n",
        "            print(\"Label \" + l)\n",
        "            sup_label = l.split('_')[0]\n",
        "            print(\"SupLabel \" + sup_label)\n",
        "            if l in all_labels:\n",
        "              index_in_labels = all_labels.index(l)\n",
        "            # and set to one\n",
        "              y[index_in_labels] = 1\n",
        "\n",
        "        print('Y vector: ', y, 'for class', label)\n",
        "\n",
        "        x_vectors.append(x)\n",
        "        y_labels.append(y)\n",
        "        print(\"********* END ***********\")\n",
        "\n",
        "    # replace all word indices larger than nb_words with OOV\n",
        "    if max_words:\n",
        "        x_vectors = [[2 if word_index >= max_words else word_index for word_index in x] for x in x_vectors]\n",
        "\n",
        "    train_instances = x_vectors\n",
        "    train_labels = y_labels\n",
        "\n",
        "    return train_instances, train_labels, id_vector\n",
        "\n",
        "\n",
        "def load_csv_data(directory, test_split=0.2, max_words=None, reduced_label_set=False):\n",
        "    files = listdir(directory)\n",
        "    folders = dict()\n",
        "    for file_name in files:\n",
        "        training_file_names = copy(files)\n",
        "        training_file_names.remove(file_name)\n",
        "        folders[file_name] = {\"training\": training_file_names, \"test\": file_name}\n",
        "\n",
        "    word_to_indices_map, word_index_to_embeddings_map = load_wordembedding_file()\n",
        "\n",
        "\n",
        "    # results: map with fold_name (= file_name) and two tuples: (train_x, train_y), (test_x, test_y)\n",
        "    output_folder_with_train_test_data = dict()\n",
        "\n",
        "    # load all data first\n",
        "    all_loaded_files = dict()\n",
        "    for file_name in folders.keys():\n",
        "        test_instances, test_labels, ids = load_csv(directory, file_name, word_to_indices_map, max_words,\n",
        "                                                            reduced_label_set)\n",
        "        all_loaded_files[file_name] = test_instances, test_labels, ids\n",
        "    print(\"Loaded\", len(all_loaded_files), \"files\")\n",
        "\n",
        "    # parse each csv file in the directory\n",
        "    for file_name in folders.keys():\n",
        "        \n",
        "        output_folder_with_train_test_data[file_name] = dict()\n",
        "        current_folder = output_folder_with_train_test_data[file_name]\n",
        "        test_instances, test_labels, ids, = all_loaded_files.get(file_name)\n",
        "        current_folder[\"test\"] = test_instances, test_labels, ids\n",
        "\n",
        "        # now collect all training instances\n",
        "        all_training_instances = []\n",
        "        all_training_labels = []\n",
        "        all_training_ids = []\n",
        "        for training_file_name in folders.get(file_name)[\"training\"]:\n",
        "            training_instances, training_labels, ids = all_loaded_files.get(training_file_name)\n",
        "            all_training_instances.extend(training_instances)\n",
        "            all_training_labels.extend(training_labels)\n",
        "            all_training_ids.extend(ids)\n",
        "\n",
        "        current_folder[\"training\"] = all_training_instances, all_training_labels, all_training_ids\n",
        "\n",
        "    return output_folder_with_train_test_data, word_index_to_embeddings_map\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZqV898w4iFuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIeBKvyQqK_z",
        "outputId": "6a37d5f0-1f04-4506-c9f3-2ec6ba871851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "def get_label_from_vector(vector):\n",
        "    all_labels = [\"o5_1\", \"o5_2\", \"o5_3\", \"o6_1\", \"o6_2\", \"o6_3\", \"o7_1\", \"o7_2\", \"o7_3\", \"o7_4\", \"o8_1\", \"o8_4\",\"o8_5\", \"o9_1\", \"o9_2\", \"o9_3\", \"o9_4\"]\n",
        "    max_value_index = np.argmax(np.array(vector))\n",
        "    return all_labels[max_value_index]\n",
        "\n",
        "def get_model(X_train, y_train, embeddings, batch_size, epochs, max_len, max_features, output_vals=17):\n",
        "\n",
        "    print(\"___ Deveoloping Nueral Network Architecture__\")\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, activation='relu', input_shape=(max_len,)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(output_vals))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_crossentropy'])\n",
        "    model.fit(X_train,y_train, batch_size, epochs, 0.1,0)\n",
        "    return model\n",
        "\n",
        "\n",
        "def run__main__():\n",
        "    np.random.seed(1000)  # for reproducibility\n",
        "    max_features = 20000\n",
        "    max_len = 300 # cut texts after this number of words (among top max_features most common words)\n",
        "    batch_size = 20\n",
        "    epochs = 5  # 5 epochs are meaningful to prevent over-fitting...\n",
        "\n",
        "    input_folder =  '/content/gdrive/MyDrive/WMCA/data/CSV-format'\n",
        "    input_folder = 'https://github.com/UKPLab/emnlp2016-empirical-convincingness/tree/master/data/CSV-format'\n",
        "\n",
        "    folders, word_index_to_embeddings_map = load_csv_data(input_folder, max_words=max_features)\n",
        "\n",
        "    all_folders_org = []\n",
        "    all_folders_predicted = []\n",
        "\n",
        "    final_results = {}\n",
        "    for folder in folders.keys():\n",
        "\n",
        "        x_matrix_train, y_matrix_train, ids_train = folders.get(folder)[\"training\"]\n",
        "        x_matrix_test, y_matrix_test, ids_test = folders.get(folder)[\"test\"]\n",
        "\n",
        "        embeddings = np.asarray([np.array(x, dtype=float32) for x in word_index_to_embeddings_map.values()])\n",
        "        # print(\"Pad sequences (samples x time)\")\n",
        "        x_matrix_train = sequence_module.pad_sequences(x_matrix_train, maxlen=max_len)\n",
        "        x_matrix_test = sequence_module.pad_sequences(x_matrix_test, maxlen=max_len)\n",
        "        # print('x_matrix_train shape:', x_matrix_train.shape)\n",
        "        # print('x_matrix_test shape:', x_matrix_test.shape)\n",
        "        y_matrix_train = np.array(y_matrix_train)\n",
        "        y_matrix_test = np.array(y_matrix_test)\n",
        "\n",
        "        \n",
        "        print('**** GETTING MODEL ***')\n",
        "        model = get_model(x_matrix_train, y_matrix_train, embeddings, batch_size, epochs, max_len,\n",
        "                          max_features)\n",
        "\n",
        "        print('**** Prediction ****')\n",
        "        model_predict = model.predict(x_matrix_test, batch_size=batch_size)\n",
        "        predicted_labels = np.round(np.array(model_predict))\n",
        "\n",
        "        # collect wrong predictions\n",
        "        wrong_predictions_ids = []\n",
        "\n",
        "        # hamming loss\n",
        "        hamming_loss = sklearn.metrics.hamming_loss(y_matrix_test, predicted_labels)\n",
        "        # one-error\n",
        "        # most probable single prediction\n",
        "        one_error_raw = 0.0\n",
        "\n",
        "        file_resutlts = {}\n",
        "        for i, (a, b) in enumerate(zip(y_matrix_test, predicted_labels)):\n",
        "            max_value_index = np.argmax(np.array(a))\n",
        "            one_error_match = np.round(b)[max_value_index] == np.round(a)[max_value_index]\n",
        "            if one_error_match:\n",
        "                one_error_raw += 1.0\n",
        "        # value\n",
        "        one_error = one_error_raw / np.array(y_matrix_test).shape[0]\n",
        "        file_resutlts['OneError'] = one_error\n",
        "        file_resutlts['HummingLoss'] = hamming_loss\n",
        "\n",
        "        print(\"One error:\", one_error, folder)\n",
        "        print(\"Hamming loss:\", hamming_loss, folder)\n",
        "\n",
        "        for i, (a, b) in enumerate(zip(y_matrix_test, predicted_labels)):\n",
        "            label_org = get_label_from_vector(a)\n",
        "            label_predicted = get_label_from_vector(b)\n",
        "            all_folders_org.append(label_org)\n",
        "            all_folders_predicted.append(label_predicted)\n",
        "\n",
        "            if a.any() != b.any():\n",
        "                wrong_predictions_ids.append(ids_test[i])\n",
        "        accuracy = keras.metrics.Accuracy()\n",
        "        acc = accuracy(y_matrix_test, predicted_labels)\n",
        "        file_resutlts['Accuracy'] = acc.numpy()\n",
        "        print('Test accuracy:', acc.numpy())\n",
        "        print('Wrong predictions:', wrong_predictions_ids)\n",
        "        \n",
        "        final_results[folder] = file_resutlts\n",
        "    cm = ConfusionMatrix(all_folders_org, all_folders_predicted)\n",
        "    \n",
        "\n",
        "    print(cm)\n",
        "    \n",
        "    # cf_matrix = confusion_matrix(y_matrix_test, predicted_labels)\n",
        "    # sns.heatmap(cf_matrix)\n",
        "\n",
        "    print(final_results)\n"
      ],
      "metadata": {
        "id": "IpP1n-ZenVbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Main"
      ],
      "metadata": {
        "id": "Q8aMgB8Z7-dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run__main__()"
      ],
      "metadata": {
        "id": "g8q5KmOfpe_N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "2302a441-11c7-4fa7-b9ff-cf0fd86f58ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7dcdd98ae66e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun__main__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-7332683322e6>\u001b[0m in \u001b[0;36mrun__main__\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0minput_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://github.com/UKPLab/emnlp2016-empirical-convincingness/tree/master/data/CSV-format'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mfolders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index_to_embeddings_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mall_folders_org\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-aa7633cc006f>\u001b[0m in \u001b[0;36mload_csv_data\u001b[0;34m(directory, test_split, max_words, reduced_label_set)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced_label_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mfolders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://github.com/UKPLab/emnlp2016-empirical-convincingness/tree/master/data/CSV-format'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flaw Detection\n"
      ],
      "metadata": {
        "id": "3fdMxm7cMmh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_from_vector(vector):\n",
        "    all_labels = [\"o5_1\", \"o5_2\", \"o5_3\", \"o6_1\", \"o6_2\", \"o6_3\", \"o7_1\", \"o7_2\", \"o7_3\", \"o7_4\"]\n",
        "    max_value_index = np.argmax(np.array(vector))\n",
        "    return all_labels[max_value_index]\n",
        "\n",
        "\n",
        "def get_model(X_train, y_train, embeddings, batch_size, nb_epoch, max_len, max_features, nb_classes):\n",
        "    \n",
        "  # get correct loss\n",
        "  loss_function = 'categorical_crossentropy'\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(512, input_shape=(max_len,)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(nb_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "  model.compile(loss=loss_function, optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  model.fit(X_train, y_train, epochs=nb_epoch, batch_size=batch_size, validation_split=0.1, verbose=0)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "VSL5Wf0SrWMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __main__():\n",
        "    np.random.seed(1337)  # for reproducibility\n",
        "    max_features = 20000\n",
        "    max_len = 300  # cut texts after this number of words (among top max_features most common words)\n",
        "    batch_size = 32\n",
        "    nb_epoch = 5  # 5 epochs are meaningful to prevent over-fitting...\n",
        "    nb_classes = 3\n",
        "\n",
        "    output_folder = \"'/content/gdrive/MyDrive/WMCA/data/sdata/results.txt\"\n",
        "    input_folder =  '/content/gdrive/MyDrive/WMCA/data/CSV-format'\n",
        "\n",
        "    folders, word_index_to_embeddings_map = load_csv_data(input_folder, max_words=max_features, reduced_label_set=True)\n",
        "\n",
        "    all_folders_org = []\n",
        "    all_folders_predicted = []\n",
        "    all_output_id_pred_lines = []\n",
        "\n",
        "    final_results = {}\n",
        "    # print statistics\n",
        "    for folder in folders.keys():\n",
        "        print(\"Fold name \", folder)\n",
        "        x_matrix_train, y_matrix_train, ids_train = folders.get(folder)[\"training\"]\n",
        "        x_matrix_test, y_matrix_test, ids_test = folders.get(folder)[\"test\"]\n",
        "\n",
        "        # converting embeddings to numpy 2d array: shape = (vocabulary_size, 300)\n",
        "        embeddings = np.asarray([np.array(x, dtype=np.float32) for x in word_index_to_embeddings_map.values()])\n",
        "\n",
        "        print(len(x_matrix_train), 'train sequences')\n",
        "        print(len(x_matrix_test), 'test sequences')\n",
        "\n",
        "        print(\"Pad sequences (samples x time)\")\n",
        "        x_matrix_train = sequence_module.pad_sequences(x_matrix_train, maxlen=max_len)\n",
        "        x_matrix_test = sequence_module.pad_sequences(x_matrix_test, maxlen=max_len)\n",
        "        print('x_matrix_train shape:', x_matrix_train.shape)\n",
        "        print('x_matrix_test shape:', x_matrix_test.shape)\n",
        "\n",
        "        y_matrix_test = np.array(y_matrix_test)\n",
        "        y_matrix_train = np.array(y_matrix_train)\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "\n",
        "        model = get_model(x_matrix_train, y_matrix_train, embeddings, batch_size, nb_epoch, max_len,\n",
        "                          max_features, nb_classes)\n",
        "\n",
        "        print('Prediction')\n",
        "        model_predict = model.predict(x_matrix_test, batch_size=batch_size)\n",
        "        predicted_labels = np.round(np.array(model_predict))\n",
        "\n",
        "        # collect wrong predictions\n",
        "        wrong_predictions_ids = []\n",
        "\n",
        "        hamming_loss = sklearn.metrics.hamming_loss(y_matrix_test, predicted_labels)\n",
        "        # one-error\n",
        "        # most probable single prediction\n",
        "        one_error_raw = 0.0\n",
        "        file_resutlts = {}\n",
        "        for i, (a, b) in enumerate(zip(y_matrix_test, predicted_labels)):\n",
        "            label_org = get_label_from_vector(a)\n",
        "            label_predicted = get_label_from_vector(b)\n",
        "            all_folders_org.append(label_org)\n",
        "            all_folders_predicted.append(label_predicted)\n",
        "            \n",
        "            max_value_index = np.argmax(np.array(a))\n",
        "            one_error_match = np.round(b)[max_value_index] == np.round(a)[max_value_index]\n",
        "            if one_error_match:\n",
        "                one_error_raw += 1.0\n",
        "            instance_id = ids_test[i]\n",
        "\n",
        "            if label_org != label_predicted:\n",
        "                wrong_predictions_ids.append(instance_id)\n",
        "\n",
        "            all_output_id_pred_lines.append(str(instance_id) + '\\t' + label_org + '\\t' + label_predicted)\n",
        "\n",
        "            file_resutlts['HummingLoss'] = hamming_loss    \n",
        "            \n",
        "        # value\n",
        "        one_error = one_error_raw / np.array(y_matrix_test).shape[0]\n",
        "        file_resutlts['OneError'] = one_error\n",
        "             \n",
        "        accuracy = keras.metrics.Accuracy()\n",
        "        acc = accuracy(y_matrix_test, predicted_labels)\n",
        "        file_resutlts['Accuracy'] = acc.numpy()\n",
        "        print('Test accuracy:', acc.numpy())\n",
        "\n",
        "        print('Wrong predictions:', wrong_predictions_ids)\n",
        "        final_results[folder] = file_resutlts\n",
        "\n",
        "    cm = ConfusionMatrix(all_folders_org, all_folders_predicted)\n",
        "    print(cm.pretty_format())\n",
        "    print(final_results)\n",
        "    f = open(output_folder, 'w')\n",
        "    for item in all_output_id_pred_lines:\n",
        "        f.write(\"%s\\n\" % item)\n"
      ],
      "metadata": {
        "id": "GddCUprqrwUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__main__()"
      ],
      "metadata": {
        "id": "vH1JizLBtlm3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}